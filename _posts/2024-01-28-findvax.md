---
layout: post
published: true
title: "Findvax.us"
tagline: "I tried to make something that had a positive societal impact exactly once"
tags: [blag, AWS, api, s3, lambda]
comments: true
category: projects
permalink: /findvax/
image: findvax_header.jpg
---

Back in the deep dark of the early COVID pandemic years, the vaccine appeared like a shining beacon on the hill in the distance leading to what would surely be our collective salvation from this plague. Except for the fact that they were nearly impossible to find let alone schedule an appointment for at first. So I decided to put my embarrassing amounts of free time and vague web development skills to a use that was actually a societal good for the first time ever. Spoiler: it went okay. This is how I made [Findvax.us](https://findvax.us). <!--more--> (*The site is no longer active but still up as a project tombstone to show what it looked like, and because I'm scared of what kind of nightmarish shit would claim the domain if I let it expire.*)

Once again this was something I worked on years ago (early 2021 when vaccines were tough to get) and have only gotten around to writing a post about now.
{: .notice .author-note}

When the vaccines were first made available to regular people, it was practically impossible to get one. Here in Massachusetts, they rolled them out by age groups, with older, more vulnerable people getting first dibs. But they were mostly being doled out at a handful of state-run clinics and rarely at some pharmacy chains, all of which had their own websites and scheduling tools. There was no centralized way to find out who had availability near you and book an appointment. 

I had recently worked on a project at my day job that involved writing a scraper to navigate part of a live website and grab a portion of HTML to save for later, so we could decommission the backend app that generated it and display the HTML as a read-only archive. 

I could smell the `synergy` from a mile away. Mostly because I hadn't gotten COVID yet.

## The Shape of It

I'll skip to the ultimate design of the whole thing since I was mostly building the plane as I tried to fly it, so a lot of these design decisions came only after running into some surprise issue and not because I had a great architecture laid out from day one. 

There are four components, each of which has its own repo under the Findvax.us GitHub group to keep things somewhat organized:

- Site: the frontend website, the real client interface to the whole thing
- Scraper: the tools that would be run automatically to go find all the availability data 
- Notify: tools for alerting users about new availability
- Data: a place to store both static data and generated outputs of the Scrapers

All of this was hosted on/run via/served by various AWS services mainly because I was terrified that it would get absolutely demolished by a billion users and smother any servers I tried to stand up on my own. I'd share the Terraform configuration yaml or whatever but I barley even knew that existed so I did everything through the UI Console. Sorry! I will attempt to mention anything relevant here but it was such an ad-hoc, trial and error situation that I can't remember how half of it is set up at this point. I now have an incredible appreciation for people who are experts in this serverless architecture stuff, especially Amazon's labyrinthine pile of services. 

This is partly why I ended up trying out [Fly.io](https://fly.io) for my [next](/fartdepot/) [projects](/slike/), that along with the vivid sensation of rats gnawing at my soul for every penny I give to Jeff Bezos. 
{: .notice .author-note}

## Findvax-Site

<i class="fa-fw fab fa-github"></i> [Component GitHub Link](https://github.com/Findvax-us/findvax-site)

This is, as you might guess from the name, the frontend portion of the project. It's probably the biggest in both scale and scope, just because it's really the connecting tissue between all the organs. Visiting the site would present you with a list of states, and each state has its own page listing the known locations and availability. Fairly straightforward if you ask me! 

I won't go into much detail on how this part is constructed, that's probably the least interesting part of all. It's based on [Alpine.js](https://alpinejs.dev) and [Tailwind CSS](https://tailwindcss.com), put together with [Webpack](https://webpack.js.org) and hosted on AWS S3. I chose Tailwind because it looked nice and didn't really feel like using Twitter Bootstrap for the sixtieth time in a row, and integrated nicely with Alpine, which I chose because I don't have a lot of experience with modern Javascript frameworks. They both work mainly through expressing style and behavior in the HTML elements themselves; Tailwind styles are applied as compositions of classes and Alpine directives are a very simple set of html attributes. Both of these lend themselves to really quick and easy prototyping that can just be compiled into production-ready stuff without much extra work. As one person trying to put together what turned out to be a pretty big project I appreciated this immensely. 

The site itself was interesting to me personally after I hadn't really done much with real, honest to god web application frontend work in years, so it was both a steep learning curve (seriously, Webpack is a terrifying thing to behold and try to scry meaning from its ancient runic symbology) and a fun tour of new stuff. Although it's probably all outdated again by now. Do we still use the `<marquee>` tag or no?

![I made the spinner so incredibly wrong](/images/findvaxspinner.gif)

"Learning curve" maybe isn't the right word for the kind of geometric disasters I was creating.
{: .notice}

### About Accessibility

A particular focus for this whole project was making sure to be as accessible as possible, given that the whole point was to help people who didn't have the time, resources, or ability to scour a bunch of different websites for appointments all day long. And since this is the actual user interface of the thing, this is where it manifests most. So to that goal I used a couple of different tools from the beginning:

**[The Braille Institute's Atkinson Hyperlegible Font](https://brailleinstitute.org/freefont)** 
:   This font is designed specifically to be, as it says on the tin, hyperlegible even to people with limited vision. It also looks great if I'm being honest, all those extra exaggerated shapes make it feel very friendly and welcoming, giving the site a comfortable and trustworthy kind of feel, while also being incredibly useful. It's a really cool mix of form and function that you don't see too often in web/app design that's more worried about looking *exciting*.
<br />
<br />

**i18n Language Internationalization with [Airbnb's Polyglot.js](https://github.com/airbnb/polyglot.js)** 
:   To be perfectly honest I had no idea i18n was *a thing* until I encountered it in my day job, but I'm glad it is. For anyone where I was: [i18n is an abbreviation for "internationalization"](https://www.w3.org/International/i18n-drafts/nav/about) that represents a semi-standard effort to make web assets readily adaptable to users in their own language and culture. For this project, I was only ever planning to support the US so the cultural/localization standard isn't as relevant, but I did want to make a point of offering languages if possible. So from the start I used the extremely tiny and easy to use Polyglot library to make all the UI text translatable. Unfortunately for me, I only really speak one language and never got far enough to find volunteers to add more. 
<br />
<br />

There was always more to do as I mentioned in the README. I was reading up on a11y, accessibility in terms of things like making the page elements legible and interactive to screen readers and other assistive tools but never got that far. This stuff has come a long way from barely being an afterthought years ago, but it's still unfortunate that it's on each individual developer to have to make a point to include or develop for these interfaces and tools. It's easy to see how it gets left behind when you're rushing to even get a working app put together in the first place.

## Findvax-Scraper

<i class="fa-fw fab fa-github"></i> [Component GitHub Link](https://github.com/Findvax-us/findvax-scraper)

The scraper is the real guts of this thing, the machine that generates all the useful data that anybody is coming here to see. It's also where I started to even see if I could make this work, so it's the least well structure and commented code out of the whole mess. Calling it `A Scraper` is not an entirely accurate descriptor for what it grew into, but that's what it started as and names are hard after all. 

The architecture of the scraping tool itself is pretty straightforward. 

You may be noticing something of a trend here where I forget that the point of posts like these is to explain how they work and what I did and instead try hand wave it all away as not that interesting. This is the battle in my head between "I should put more stuff on my website" and "I don't think anyone is interested in anything I have to say."
{: .notice .author-note}

A big config json lists each known real world location with a few key details, a bunch of stuff for the UI to display about it like the address, and three keys for the scraper tools: `scraperClass`: the specific brand of scraper to use (more on that in a bit), `scraperUrl`: the location where we find the data we need (or at least the starting point to getting there), and `scraperParams`: a way to pass extra info about this location to the particular class of scraper it's using. 

The scrapers are all extensions of a base class which is made to handle the ideal simplest possible case that never actually occurs in reality:

1. Make a GET request to `scraperUrl`, follow any redirects, set cookies as directed, etc
2. Parse the response body for the information we want, a list of specific times when appointments are available
3. Format this into a neat json identifying the location by uuid and fetched timestamp

Of course that second step is going to be extremely dependent on the particular site we're accessing; this usually means parsing some HTML tables or other pretty displays to find dates and times that we need. But that's not all, some sites will also complicate that first step by requiring answering questions first or handling a waiting room to keep the site from being overloaded. This is where our subclasses come in; for example, we can write a class that overrides `scrape()` and `parse()` to get data from Stop & Shop or Hannaford websites (both of which use the same frontend software), and any locations at either just need to specify their `scraperClass` as `stopnshop` to reuse it.

Our fleet of scrapers is ready, we just need some infrastructure to actually run them and capture the results. This will be a cron invoking an AWS Lambda function on a schedule (I started with every 15 minutes but found that the performance was quick and cheap enough to get down to about 3), using [scrape.js](https://github.com/Findvax-us/findvax-scraper/blob/master/src/scrape.js#L280) as our wrapper for various tasks. We'll grab the current list of location config json from S3, kick off an asynchronous run of a Scraper instance for each of them, and write the results back to S3 for the site to serve up immediately. You'll also notice a few other helper functions in there like setting up logging and metrics like CPU and memory usage. Both turned out to be invaluable for squashing bugs later on, so I'm glad I left room for them. 

![Graphs showing runtime of scraper tasks occasionally out of control, correlating with higher failures in the overall scraper runs, until a fix was applied and the graphs level off with almost 100% success.](/images/findvax_graph_bugfix.png)

Words cannot convey the incredible hit of dopamine I got when I fixed the issue that cleaned up these graphs. 
{: .notice}

## Findvax-Data

<i class="fa-fw fab fa-github"></i> [Component GitHub Link](https://github.com/Findvax-us/findvax-data)

The most straightforward of all, and I mean it this time, findvax-data is a repository for each state's static data, like locations (that big config json I mentioned in the scraper section above), a mapping of ZIP Codes to lat/lng locations (for estimating distances on the frontend when searching for sites nearby a given location), and a list of state-specific COVID resources to link to on their page. 

The state S3 buckets are also the targets for the compiled scraper output, in the form of `availability.js`, read by the site to show which locations have, you guessed it, any availability.

This particular component was mainly an exercise in understanding the various access levels for publicly hosting data in S3 and effective caching settings, along with maybe even more importantly, automatic cache invalidation whenever the data changes. 

## Findvax-Notify

<i class="fa-fw fab fa-github"></i> [Component GitHub Link](https://github.com/Findvax-us/findvax-notify)

Finally, we have the component that I intended to set this particular attempt at vaccine availability aggregation apart from others, the option to give us your phone number for immediate text alerts whenever a selected location has availability. This one was almost complicated enough to be its own little standalone application just in sheer number of AWS services glued together to make it work.

API Gateway lets us set up a nice REST interface to trigger the Lambda script, which in turn will then store the requested notification details in a json blob in DynamoDB. The same Lambda can also be invoked at the end of our scraper runs when they update availability, so we can find any stored notification requests for locations that now have availability and send those messages via Pinpoint SMS. 

In order to keep the number of messages to a minimum, both to spare the sanity of subscribed users and my wallet, we engage in [a little bit of preprocessing](https://github.com/Findvax-us/findvax-notify/blob/master/index.js#L206) to concatenate all the newly available locations into a single text per user.

![Example SMS message showing links to several newly available locations](/images/findvax_sms.png)

## So, how did it turn out?

I created a dashboard showing as much data as I could gather about performance and usage, focused mainly on assuaging my continuing paranoia that I'd wake up to a bill from Jeff for more than the value of my car. 

![Metrics dashboard for all the Findvax services](/images/findvax_graphs.png)

This let me get a pretty good idea of how many people were visiting the site, requesting notifications, and ultimately receiving them. Despite all these metrics it's impossible to know whether anyone got a shot in their arm as a direct result of using the site or getting a notification from it. Well, except in the one case where someone sent the generic site contact address a very short two line email saying "thanks for helping me find a shot for my mom." And at the risk of sounding like a total sap, that was the best feeling I had ever gotten as a result of something I'd worked on. Yes, even better than seeing the Scraper Success graph normalize at 100%.

## After All

I had also spent almost a year at this point hiding in my apartment safely working from home and watching The Horrors unfold around me without being able to do anything about it, so this whole thing was as much a distraction for myself as it was an attempt to make something genuinely helpful. And knowing that it did in fact help at least one person, I think I can count it as a total success. I guess I also learned a bunch about asynchronous tasks, services at scale, AWS in general, and that I should be confident in my abilities after being able to throw something like this together in the span of about a month and a half, sure. But that was a *really* great email.